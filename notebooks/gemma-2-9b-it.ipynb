{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "pkgs = [\n",
    "    \"transformers\",\n",
    "    \"bitsandbytes\",\n",
    "    \"tqdm\",\n",
    "    \"accelerate\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs)\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"flash-attn\", \"--no-build-isolation\"], check=True)\n",
    "\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH   = \"../data/Electronics_sample.csv\"\n",
    "DATASET_BASE_NAME = \"Electronics_sample\"  \n",
    "TEXT_COL    = \"text\"  # Column containing the reviews/comments text                       \n",
    "CONTENT_TYPE   = \"electronics\"\n",
    "OUTPUT_DIR  = \"../output\"\n",
    "MODEL_NAME  = \"google/gemma-2-9b-it\"\n",
    "PROMPT_PATH = \"../prompt.txt\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.head(100).copy()\n",
    "\n",
    "with open(PROMPT_PATH, 'r') as f:\n",
    "    instruction = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6602245",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"cuda\",\n",
    "    dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model_chat(system_prompt: str, user_prompt: str) -> str:\n",
    "    merged_user_content = f\"{system_prompt.strip()}\\n\\n: {user_prompt.strip()}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": merged_user_content}]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    end_turn_id = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    if end_turn_id is None or end_turn_id < 0:\n",
    "        end_turn_id = tokenizer.eos_token_id\n",
    "\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=900,\n",
    "        min_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        eos_token_id=end_turn_id,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, generation_config=gen_cfg)\n",
    "\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    text = tokenizer.decode(gen_ids, skip_special_tokens=False).strip()\n",
    "    text = text.replace(\"```json\", \"\")\n",
    "    text = text.replace(\"```<end_of_turn>\", \"\")\n",
    "    text = text.replace(\"<end_of_turn>\", \"\")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = common.run_inference(\n",
    "    df=df,\n",
    "    text_col=TEXT_COL,\n",
    "    tokenizer=tokenizer,\n",
    "    run_local_model_chat=run_local_model_chat,\n",
    "    instruction=instruction,\n",
    "    content_type=CONTENT_TYPE,\n",
    "    MODEL_NAME=MODEL_NAME,\n",
    "    system=\"You are an opinion mining assistant.\",\n",
    "    MAX_TOKENS= 7500,\n",
    "    SAFE_TOKENS= 5000,\n",
    "    max_attempts= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tuples = common.build_pred_tuples(response_list)\n",
    "common.save_pred_tuples_to_pickle(OUTPUT_DIR, MODEL_NAME, pred_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdfcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = common.add_tuples_to_df(df, response_list)\n",
    "common.save_dataset(updated_df, OUTPUT_DIR, MODEL_NAME, DATASET_BASE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
