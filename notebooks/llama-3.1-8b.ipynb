{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "pkgs = [\n",
    "    \"transformers\",\n",
    "    \"bitsandbytes\",\n",
    "    \"tqdm\",\n",
    "    \"accelerate\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs)\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"flash-attn\", \"--no-build-isolation\"], check=True)\n",
    "\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7543e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH   = \"../data/Electronics_sample.csv\"\n",
    "DATASET_BASE_NAME = \"Electronics_sample\"  \n",
    "TEXT_COL    = \"text\"  # Column containing the reviews/comments text                       \n",
    "CONTENT_TYPE   = \"electronics\"\n",
    "OUTPUT_DIR  = \"../output\"\n",
    "MODEL_NAME  = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "PROMPT_PATH = \"../prompt.txt\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.head(100).copy()\n",
    "\n",
    "with open(PROMPT_PATH, 'r') as f:\n",
    "    instruction = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def run_local_model_chat(system_prompt: str, user_prompt: str) -> str:\n",
    "    gen_cfg = GenerationConfig(max_new_tokens=900, do_sample=False)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    enc = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            generation_config=gen_cfg,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_only = out[0][enc[\"input_ids\"].shape[1]:] # Only the answer\n",
    "    return tokenizer.decode(gen_only, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5522e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = common.run_inference(\n",
    "    df=df,\n",
    "    text_col=TEXT_COL,\n",
    "    tokenizer=tokenizer,\n",
    "    run_local_model_chat=run_local_model_chat,\n",
    "    instruction=instruction,\n",
    "    content_type=CONTENT_TYPE,\n",
    "    MODEL_NAME=MODEL_NAME,\n",
    "    system=\"You are an opinion mining assistant.\",\n",
    "    MAX_TOKENS= 7500,\n",
    "    SAFE_TOKENS= 5000,\n",
    "    max_attempts= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tuples = common.build_pred_tuples(response_list)\n",
    "common.save_pred_tuples_to_pickle(OUTPUT_DIR, MODEL_NAME, pred_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = common.add_tuples_to_df(df, response_list)\n",
    "common.save_dataset(updated_df, OUTPUT_DIR, MODEL_NAME, DATASET_BASE_NAME)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
